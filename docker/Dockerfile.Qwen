# 使用PyTorch官方CUDA镜像作为基础镜像,是3.10.13的
FROM pytorch/pytorch:2.2.0-cuda11.8-cudnn8-runtime
RUN python3 --version


# 得关闭进度条不然会报错
RUN pip config --user set global.progress_bar off
# 安装fschat及其依赖项
RUN pip3 install fschat "fschat[model_worker,webui]" -i https://pypi.mirrors.ustc.edu.cn/simple/ --no-cache-dir
RUN pip3 install pydantic -i https://pypi.mirrors.ustc.edu.cn/simple/ --no-cache-dir
RUN pip3 install pip install tiktoken einops transformers_stream_generator -i https://pypi.mirrors.ustc.edu.cn/simple/ --no-cache-dir
# RUN pip install https://github.com/vllm-project/vllm/releases/download/v0.3.1/vllm-0.3.1+cu118-cp310-cp310-manylinux1_x86_64.whl
RUN pip install accelerate -i https://pypi.mirrors.ustc.edu.cn/simple/ --no-cache-dir

# 复制启动脚本到镜像
COPY ./docker/start_services.sh /start_services.sh

# 给启动脚本执行权限
RUN chmod +x /start_services.sh

# 复制模型到镜像
COPY ./models/Qwen-1_8B-Chat /models/Qwen-1_8B-Chat


# 设置环境变量
ENV FASTCHAT_CONTROLLER_ADDRESS=localhost
ENV FASTCHAT_CUDA_DEVICE=7
ENV FASTCHAT_API_KEYS=123456,12345678
ENV FASTCHAT_WORKER_MODEL_NAMES=Qwen-1_8B-Chat
ENV FASTCHAT_WORKER_MODEL_PATH=/models/Qwen-1_8B-Chat


# 暴露需要的端口
EXPOSE 21001 21002 8000


# 设置入口点为 start_services.sh 脚本
# ENTRYPOINT ["/start_services.sh"]

# sudo docker build -f ./docker/Dockerfile.Qwen -t fastchat-image .
# 构建启动命令 sudo docker run -d -p 21001:21001 -p 21002:21002 -p 8000:8000 --name fastchat-container fastchat-image
# 构建启动命令 sudo docker run -d -p 23002:21001 -p 23002:21002 -p 8000:8000 --name fastchat-container fastchat-image:latest
# 构建启动命令 sudo docker run -it --gpus all -d -p 8085:8000 --name fastchat-container fastchat-image:latest
# sudo docker logs -f fastchat-container
