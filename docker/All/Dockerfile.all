# 使用PyTorch官方CUDA镜像作为基础镜像,是3.10.13的
FROM pytorch/pytorch:2.1.0-cuda11.8-cudnn8-runtime
# FROM nvcr.io/nvidia/cuda:12.1.0-devel-ubuntu22.04 # python3 都没装
# ARG BUILD_DATE=unknown
# RUN echo "Build date: ${BUILD_DATE}"
# RUN python3 --version
WORKDIR /app
COPY pyproject.toml \
    .pylintrc /app/
COPY docker /app/docker/
COPY fastchat /app/fastchat/
COPY tests /app/tests/
COPY wheel_cache/xformers-0.0.22.post4+cu118-cp310-cp310-manylinux2014_x86_64.whl  \
    wheel_cache/xformers-0.0.23.post1-cp310-cp310-manylinux2014_x86_64.whl  \
    wheel_cache/torch-2.1.0+cu118-cp310-cp310-linux_x86_64.whl \
    wheel_cache/torch-2.1.2+cu118-cp310-cp310-linux_x86_64.whl  \
    wheel_cache/cupy_cuda12x-12.1.0-cp310-cp310-manylinux2014_x86_64.whl  \
    wheel_cache/pynvml-11.5.0-py3-none-any.whl  \
    wheel_cache/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl  \
    wheel_cache/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl  \
    wheel_cache/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl  \
    wheel_cache/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl  \
    wheel_cache/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl  \
    wheel_cache/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl  \
    wheel_cache/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl  \
    wheel_cache/nvidia_nvjitlink_cu12-12.1.105-py3-none-manylinux1_x86_64.whl \
    wheel_cache/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl \
    wheel_cache/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl /app/wheel_cache/

# 设置pip源为清华大学镜像源
ENV PIP_INDEX_URL=https://pypi.tuna.tsinghua.edu.cn/simple
# # 得关闭进度条不然会报错
# 合并RUN指令
RUN pip config --user set global.progress_bar off && \
    pip install --upgrade pip && \
    pip3 install -e "." && \
    pip3 install "transformers==4.39.1" einops transformers_stream_generator==0.0.5 accelerate peft sentencepiece protobuf autoawq --no-cache-dir && \
    pip3 install "fastrlock>=0.5" --no-cache-dir

# Downloading cupy_cuda12x-12.1.0-cp310-cp310-manylinux2014_x86_64.whl (83.0 MB)
# Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)
# Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)
# Downloading xformers-0.0.23.post1-cp310-cp310-manylinux2014_x86_64.whl (213.0 MB)
# Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)
# Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)
# Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)
# Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)
# Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)
# Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)
# Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)
# Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)
# Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)
WORKDIR /app
RUN pip3 install --no-index --find-links=./wheel_cache ./wheel_cache/cupy_cuda12x-12.1.0-cp310-cp310-manylinux2014_x86_64.whl && \
    pip3 install --no-index --find-links=./wheel_cache ./wheel_cache/torch-2.1.2+cu118-cp310-cp310-linux_x86_64.whl && \
    pip3 install --no-index --find-links=./wheel_cache ./wheel_cache/pynvml-11.5.0-py3-none-any.whl && \
    pip3 install --no-index --find-links=./wheel_cache ./wheel_cache/xformers-0.0.23.post1-cp310-cp310-manylinux2014_x86_64.whl && \
    pip3 install --no-index --find-links=./wheel_cache ./wheel_cache/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl && \
    pip3 install --no-index --find-links=./wheel_cache ./wheel_cache/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl && \
    pip3 install --no-index --find-links=./wheel_cache ./wheel_cache/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl && \
    pip3 install --no-index --find-links=./wheel_cache ./wheel_cache/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl && \
    pip3 install --no-index --find-links=./wheel_cache ./wheel_cache/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl && \
    pip3 install --no-index --find-links=./wheel_cache ./wheel_cache/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl && \
    pip3 install --no-index --find-links=./wheel_cache ./wheel_cache/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl && \
    pip3 install --no-index --find-links=./wheel_cache ./wheel_cache/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl
    # pip3 install --no-index --find-links=./wheel_cache ./wheel_cache/nvidia_nvjitlink_cu12-12.1.105-py3-none-manylinux1_x86_64.whl && \
    # pip3 install --no-index --find-links=./wheel_cache ./wheel_cache/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl


# 得在线装,有很多依赖,否则全部得一个一个找下载到本地提前安装
# https://github.com/vllm-project/vllm/releases
RUN pip3 install https://github.com/vllm-project/vllm/releases/download/v0.3.3/vllm-0.3.3+cu118-cp310-cp310-manylinux1_x86_64.whl && \
    pip3 install --no-index --find-links=./wheel_cache ./wheel_cache/torch-2.1.0+cu118-cp310-cp310-linux_x86_64.whl && \
    pip3 install --no-index --find-links=./wheel_cache ./wheel_cache/xformers-0.0.22.post4+cu118-cp310-cp310-manylinux2014_x86_64.whl && \
    pip3 install pydantic==1.10.11 --no-cache-dir && \
    # 删除不必要的文件
    rm -rf /app/wheel_cache && \
    chmod +x /app/docker/All/start_all.sh && \
    python3 ./tests/test_cuda.py
    # pydantic高版本弃用了dumps_kwargs 
# RUN pip3 install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu118

# 设置环境变量
# Controller地址
ENV FASTCHAT_CONTROLLER_ADDRESS=192.168.190.79 \
    # Controller端口
    FASTCHAT_CONTROLLER_PORT=22001 \
    # OpenAI API端口
    FASTCHAT_OPENAI_API_PORT=28000 \
    # 日志级别
    LOG_LEVEL='DEBUG' \
    # 日志级别
    FASTCHAT_LOG_LEVEL=debug \
    # Worker地址
    FASTCHAT_WORKER_ADDRESS=192.168.190.79 \
    # Worker端口
    FASTCHAT_WORKER_PORT=22002 \
    # Worker的CUDA设备
    FASTCHAT_DOCKER_CUDA_DEVICE=0,1 \
    # Worker的GPU数量
    NUM_GPUS=2 \
    # Worker的推理方式类型
    USE_WORKER=VLLM_WORKER \
    # Worker的模型名称
    FASTCHAT_WORKER_MODEL_NAMES=Qwen-1_8B-Chat \
    # Worker的模型路径
    FASTCHAT_WORKER_MODEL_PATH=/app/models/ \
    FASTCHAT_CONV_TEMPLATE= \
    # 现存占用百分比
    GPU_MEMORY_UTILIZATION=0.85 \
    # 最大模型长度
    MAX_MODEL_LEN=1024 \
    # 限制工作线程
    LIMIT_WORKER=5 \
    RAY_USE_MULTIPROCESSING_CPU_COUNT= \
    RAY_DISABLE_DOCKER_CPU_WARNING=1 \
    RAY_memory_monitor_refresh_ms=0 


# # 暴露需要的端口
EXPOSE 22001 28000 22002


ENTRYPOINT ["./docker/All/start_all.sh"]
# sudo docker build  --build-arg BUILD_DATE=$(date +%Y-%m-%d:%H:%M:%S) -t fastchat-worker:0.2 -f ./docker/Worker/Dockerfile.Worker .